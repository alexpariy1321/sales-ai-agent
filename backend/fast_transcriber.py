# –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ç—á –î–û –∏–º–ø–æ—Ä—Ç–∞ whisperx
from backend.torch_patch import patched_load
import torch
torch.load = patched_load

import whisperx
from pyannote.audio import Pipeline
import gc
import os
import warnings
from dotenv import load_dotenv

warnings.filterwarnings("ignore", category=UserWarning)
load_dotenv()

class FastTranscriber:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å–∫—Ä–∞–π–±–µ—Ä —Å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–µ–π —á–µ—Ä–µ–∑ pyannote"""
    
    def __init__(self, model_name="tiny", device="cpu"):
        self.device = device
        self.model_name = model_name
        
        # –ò—â–µ–º —Ç–æ–∫–µ–Ω
        self.hf_token = (
            os.getenv("HF_TOKEN") or 
            os.getenv("HUGGINGFACE_TOKEN") or 
            os.getenv("HFTOKEN")
        )
        
        print(f"üöÄ –ó–∞–≥—Ä—É–∂–∞–µ–º WhisperX –º–æ–¥–µ–ª—å '{model_name}'...")
        self.model = whisperx.load_model(
            model_name, 
            device=device, 
            compute_type="int8",
            language="ru"
        )
        
        print("üéØ –ó–∞–≥—Ä—É–∂–∞–µ–º align –º–æ–¥–µ–ª—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ...")
        self.align_model, self.align_metadata = whisperx.load_align_model(
            language_code="ru", 
            device=device
        )
        
        print("üë• –ó–∞–≥—Ä—É–∂–∞–µ–º diarization –º–æ–¥–µ–ª—å...")
        if self.hf_token:
            try:
                print(f"   –¢–æ–∫–µ–Ω –Ω–∞–π–¥–µ–Ω: {self.hf_token[:15]}...")
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º pyannote –Ω–∞–ø—Ä—è–º—É—é
                self.diarize_model = Pipeline.from_pretrained(
                    "pyannote/speaker-diarization-3.1",
                    use_auth_token=self.hf_token
                )
                self.diarize_model.to(torch.device(device))
                print("   ‚úÖ –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –≤–∫–ª—é—á–µ–Ω–∞!")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏: {str(e)[:100]}")
                self.diarize_model = None
        else:
            self.diarize_model = None
            print("   ‚ö†Ô∏è  HF_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env")
        
        print("‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!\n")
    
    def transcribe_with_speakers(self, audio_path: str) -> dict:
        """
        –ë—ã—Å—Ç—Ä–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è —Å –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–µ–π
        """
        try:
            # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ
            audio = whisperx.load_audio(audio_path)
            
            # 2. –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è
            result = self.model.transcribe(audio, batch_size=16)
            
            # 3. Align
            result = whisperx.align(
                result["segments"], 
                self.align_model, 
                self.align_metadata, 
                audio, 
                self.device,
                return_char_alignments=False
            )
            
            # 4. –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ pyannote
            if self.diarize_model:
                try:
                    from pyannote.core import Segment
                    import numpy as np
                    
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∞—É–¥–∏–æ –¥–ª—è pyannote
                    audio_tensor = torch.from_numpy(audio).unsqueeze(0)
                    waveform = {
                        "waveform": audio_tensor,
                        "sample_rate": 16000
                    }
                    
                    # –ó–∞–ø—É—Å–∫–∞–µ–º –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—é
                    diarization = self.diarize_model(waveform)
                    
                    # –ü—Ä–∏–≤—è–∑—ã–≤–∞–µ–º —Å–ø–∏–∫–µ—Ä–æ–≤ –∫ —Å–ª–æ–≤–∞–º
                    for segment in result["segments"]:
                        segment_start = segment["start"]
                        segment_end = segment["end"]
                        
                        # –ù–∞—Ö–æ–¥–∏–º –¥–æ–º–∏–Ω–∏—Ä—É—é—â–µ–≥–æ —Å–ø–∏–∫–µ—Ä–∞ –≤ —ç—Ç–æ–º —Å–µ–≥–º–µ–Ω—Ç–µ
                        speakers_in_segment = []
                        for turn, _, speaker in diarization.itertracks(yield_label=True):
                            if turn.start < segment_end and turn.end > segment_start:
                                overlap = min(turn.end, segment_end) - max(turn.start, segment_start)
                                speakers_in_segment.append((speaker, overlap))
                        
                        if speakers_in_segment:
                            # –ë–µ—Ä–µ–º —Å–ø–∏–∫–µ—Ä–∞ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –≤—Ä–µ–º–µ–Ω–µ–º –≤ —Å–µ–≥–º–µ–Ω—Ç–µ
                            dominant_speaker = max(speakers_in_segment, key=lambda x: x[1])[0]
                            segment["speaker"] = f"SPEAKER_{dominant_speaker}"
                        else:
                            segment["speaker"] = "SPEAKER_00"
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è  –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞: {str(e)[:80]}")
                    # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –≤—Å–µ–º SPEAKER_00
                    for segment in result["segments"]:
                        segment["speaker"] = "SPEAKER_00"
            else:
                # –ë–µ–∑ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ - –≤—Å–µ SPEAKER_00
                for segment in result["segments"]:
                    segment["speaker"] = "SPEAKER_00"
            
            # 5. –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            full_text = " ".join([seg.get("text", "") for seg in result["segments"]])
            
            speakers_text = []
            for seg in result["segments"]:
                speaker = seg.get("speaker", "SPEAKER_00")
                text = seg.get("text", "")
                if text.strip():
                    speakers_text.append({
                        "speaker": speaker,
                        "text": text.strip(),
                        "start": seg.get("start", 0),
                        "end": seg.get("end", 0)
                    })
            
            return {
                "transcript": full_text.strip(),
                "speakers": speakers_text,
                "duration": audio.shape[0] / 16000,
                "num_speakers": len(set([s['speaker'] for s in speakers_text])),
                "status": "success"
            }
        except Exception as e:
            return {
                "transcript": "",
                "speakers": [],
                "duration": 0,
                "num_speakers": 0,
                "status": "error",
                "error": str(e)
            }
    
    def cleanup(self):
        """–û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å"""
        try:
            del self.model
            del self.align_model
            if self.diarize_model:
                del self.diarize_model
            gc.collect()
        except:
            pass

def transcribe_audio(audio_path: str) -> dict:
    """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –≤—ã–∑–æ–≤–∞"""
    transcriber = FastTranscriber(model_name="tiny")
    result = transcriber.transcribe_with_speakers(audio_path)
    transcriber.cleanup()
    return result
