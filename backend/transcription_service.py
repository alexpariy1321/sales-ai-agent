import os
import requests
import whisperx
import torch
import gc
import subprocess
from pyannote.audio import Pipeline
from dotenv import load_dotenv

load_dotenv()

_whisper_model = None
_diarize_model = None

def get_whisper_model():
    global _whisper_model
    if _whisper_model is None:
        print("–ó–∞–≥—Ä—É–∂–∞–µ–º Whisper MEDIUM (—Ç–æ—á–Ω–µ–µ base)...")
        _whisper_model = whisperx.load_model(
            "medium",  # ‚Üê –ë—ã–ª–æ base, —Ç–µ–ø–µ—Ä—å medium
            device="cpu",
            compute_type="int8",
            language="ru"
        )
        print("‚úÖ Whisper medium –≥–æ—Ç–æ–≤")
    return _whisper_model

def get_diarize_model():
    global _diarize_model
    if _diarize_model is None:
        hf_token = os.getenv("HF_TOKEN")
        if not hf_token:
            raise ValueError("HF_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        print("–ó–∞–≥—Ä—É–∂–∞–µ–º pyannote –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—é 3.1...")
        _diarize_model = Pipeline.from_pretrained(
            "pyannote/speaker-diarization-3.1",
            use_auth_token=hf_token
        )
        _diarize_model.to(torch.device("cpu"))
        print("‚úÖ Diarization –≥–æ—Ç–æ–≤")
    return _diarize_model

def fix_audio_with_ffmpeg(input_path: str, output_path: str) -> bool:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç MP3 ‚Üí WAV —á–µ—Ä–µ–∑ ffmpeg"""
    try:
        subprocess.run([
            "ffmpeg",
            "-i", input_path,
            "-ar", "16000",
            "-ac", "1",
            "-y",
            output_path
        ], check=True, capture_output=True)
        return True
    except subprocess.CalledProcessError:
        return False

def transcribe_with_diarization(audio_url: str, num_speakers: int = 2) -> dict:
    temp_mp3_path = None
    temp_wav_path = None
    
    try:
        print(f"üì• –°–∫–∞—á–∏–≤–∞–µ–º –∞—É–¥–∏–æ...")
        response = requests.get(audio_url, timeout=60)
        
        if response.status_code != 200:
            return {"status": "error", "error": f"HTTP {response.status_code}"}
        
        temp_mp3_path = "/tmp/temp_call.mp3"
        temp_wav_path = "/tmp/temp_call.wav"
        
        with open(temp_mp3_path, "wb") as f:
            f.write(response.content)
        
        print(f"‚úÖ {len(response.content)/1024/1024:.2f} MB")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ ffmpeg
        print("üîß –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º MP3‚ÜíWAV...")
        if not fix_audio_with_ffmpeg(temp_mp3_path, temp_wav_path):
            return {"status": "error", "error": "–û—à–∏–±–∫–∞ ffmpeg"}
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º WAV
        print("üéß –ó–∞–≥—Ä—É–∑–∫–∞...")
        audio = whisperx.load_audio(temp_wav_path)
        
        duration = len(audio) / 16000
        print(f"‚è±Ô∏è  –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {duration:.1f}—Å")
        
        # –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è (medium –º–æ–¥–µ–ª—å)
        print("üìù Whisper MEDIUM —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è...")
        model = get_whisper_model()
        result = model.transcribe(audio, batch_size=8)  # batch=8 –¥–ª—è medium
        
        # –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∑–≤–æ–Ω–∫–æ–≤
        use_diarization = duration > 20
        
        if use_diarization:
            try:
                print("üë• Pyannote –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è (2 —Å–ø–∏–∫–µ—Ä–∞)...")
                diarize_model = get_diarize_model()
                
                diarize_result = diarize_model(
                    temp_wav_path,
                    min_speakers=num_speakers,
                    max_speakers=num_speakers
                )
                
                # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º —Å–ø–∏–∫–µ—Ä–æ–≤ –∫ —Å–µ–≥–º–µ–Ω—Ç–∞–º
                segments = []
                for seg in result.get("segments", []):
                    start = seg.get("start", 0)
                    end = seg.get("end", start + 1)
                    text = seg.get("text", "").strip()
                    
                    # –ù–∞—Ö–æ–¥–∏–º —Å–ø–∏–∫–µ—Ä–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –º–µ—Ç–∫–µ
                    speaker = "UNKNOWN"
                    max_overlap = 0
                    
                    for turn, _, spk in diarize_result.itertracks(yield_label=True):
                        # –í—ã—á–∏—Å–ª—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
                        overlap_start = max(start, turn.start)
                        overlap_end = min(end, turn.end)
                        overlap = max(0, overlap_end - overlap_start)
                        
                        if overlap > max_overlap:
                            max_overlap = overlap
                            speaker = spk
                    
                    if text:
                        time_str = f"{int(start // 60):02d}:{int(start % 60):02d}"
                        segments.append({
                            "speaker": speaker,
                            "time": time_str,
                            "text": text
                        })
                
                print(f"‚úÖ –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞!")
                
            except Exception as e:
                print(f"‚ö†Ô∏è  –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                use_diarization = False
        
        # –ë–µ–∑ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏
        if not use_diarization:
            segments = []
            for seg in result.get("segments", []):
                start = seg.get("start", 0)
                text = seg.get("text", "").strip()
                
                if text:
                    time_str = f"{int(start // 60):02d}:{int(start % 60):02d}"
                    segments.append({
                        "speaker": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ",
                        "time": time_str,
                        "text": text
                    })
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç
        full_transcript = []
        for seg in segments:
            full_transcript.append(f"[{seg['time']}] {seg['speaker']}: {seg['text']}")
        
        # –†–æ–ª–∏ —Å–ø–∏–∫–µ—Ä–æ–≤
        speakers_map = {}
        unique_speakers = list(set(seg["speaker"] for seg in segments))
        
        if use_diarization:
            unique_speakers = [s for s in unique_speakers if s != "UNKNOWN"]
            if len(unique_speakers) >= 1:
                speakers_map[unique_speakers[0]] = "–ú–µ–Ω–µ–¥–∂–µ—Ä"
            if len(unique_speakers) >= 2:
                speakers_map[unique_speakers[1]] = "–ö–ª–∏–µ–Ω—Ç"
        else:
            speakers_map["–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"] = "–ó–≤–æ–Ω–æ–∫ –∫–æ—Ä–æ—Ç–∫–∏–π" if duration <= 20 else "–û—à–∏–±–∫–∞ –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏"
        
        # –û—á–∏—Å—Ç–∫–∞
        del audio
        gc.collect()
        
        for path in [temp_mp3_path, temp_wav_path]:
            if path and os.path.exists(path):
                os.remove(path)
        
        print(f"‚úÖ –ì–æ—Ç–æ–≤–æ! {len(segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
        
        return {
            "status": "success",
            "transcript": "\n".join(full_transcript),
            "segments": segments,
            "speakers_map": speakers_map,
            "stats": {
                "total_segments": len(segments),
                "speakers_detected": len(unique_speakers),
                "duration": f"{duration:.1f}s",
                "diarization_used": use_diarization,
                "model": "whisper-medium + pyannote-3.1"
            }
        }
        
    except Exception as e:
        for path in [temp_mp3_path, temp_wav_path]:
            if path and os.path.exists(path):
                os.remove(path)
        
        import traceback
        return {
            "status": "error",
            "error": str(e),
            "traceback": traceback.format_exc()
        }

def get_quota_info():
    return {
        "provider": "WhisperX Medium + pyannote (–ª–æ–∫–∞–ª—å–Ω–æ)",
        "tier": "Unlimited",
        "model": "whisper-medium + speaker-diarization-3.1",
        "device": "CPU (int8)",
        "note": "Medium —Ç–æ—á–Ω–µ–µ base, –¥–∏–∞—Ä–∏–∑–∞—Ü–∏—è >20 —Å–µ–∫",
        "status": "‚úÖ –ì–æ—Ç–æ–≤"
    }
