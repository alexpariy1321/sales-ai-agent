# -*- coding: utf-8 -*-
import os
import json
import numpy as np
import torch
import torchaudio
from faster_whisper import WhisperModel
from pyannote.audio import Model, Inference
from sklearn.cluster import KMeans
from scipy.spatial.distance import cosine

# –ö–æ–Ω—Ñ–∏–≥
VOICEPRINTS_FILE = "/root/sales-ai-agent/data/voiceprints.json"
MODEL_ID = "pyannote/wespeaker-voxceleb-resnet34-LM"
HF_TOKEN = os.getenv("HF_TOKEN")

def get_embedding_model():
    print("üß† –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –≥–æ–ª–æ—Å–∞ (Pyannote)...")
    try:
        model = Model.from_pretrained(MODEL_ID, use_auth_token=HF_TOKEN)
        return Inference(model, window="whole")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        exit(1)

def extract_embeddings(audio_path, inference):
    print(f"üé§ –û–±—Ä–∞–±–æ—Ç–∫–∞: {os.path.basename(audio_path)}")
    
    # 1. Whisper –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
    print("   –ó–∞–ø—É—Å–∫ Whisper (medium)...")
    model = WhisperModel("medium", device="cpu", compute_type="int8")
    segments, _ = model.transcribe(audio_path, vad_filter=True)
    seg_list = list(segments)
    
    if not seg_list:
        print("‚ùå –°–µ–≥–º–µ–Ω—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        return None, None

    # 2. Pyannote Embedding
    print("   –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤...")
    waveform, sample_rate = torchaudio.load(audio_path)
    
    # –†–µ—Å–µ–º–ø–ª–∏–Ω–≥
    if sample_rate != 16000:
        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)
        waveform = resampler(waveform)
        sample_rate = 16000
        
    # –ú–æ–Ω–æ
    if waveform.shape[0] > 1:
        waveform = waveform.mean(dim=0, keepdim=True)

    embeddings = []
    valid_segments = []

    for seg in seg_list:
        if seg.end - seg.start < 0.5: continue
        try:
            start = int(seg.start * sample_rate)
            end = int(seg.end * sample_rate)
            if end > waveform.shape[1]: end = waveform.shape[1]
            
            chunk = waveform[:, start:end]
            emb = inference({"waveform": chunk, "sample_rate": sample_rate})
            if len(emb.shape) == 2: emb = emb[0]
            
            embeddings.append(emb)
            valid_segments.append(seg)
        except: pass

    if len(embeddings) < 2:
        print("‚ùå –ú–∞–ª–æ –≤–µ–∫—Ç–æ—Ä–æ–≤.")
        return None, None
        
    return np.stack(embeddings), valid_segments

def save_voiceprint(manager_name, embedding):
    data = {}
    if os.path.exists(VOICEPRINTS_FILE):
        try:
            with open(VOICEPRINTS_FILE, 'r') as f:
                data = json.load(f)
        except: pass
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º numpy –≤ —Å–ø–∏—Å–æ–∫
    data[manager_name] = embedding.tolist()
    
    with open(VOICEPRINTS_FILE, 'w') as f:
        json.dump(data, f)
    print(f"‚úÖ –≠—Ç–∞–ª–æ–Ω –¥–ª—è '{manager_name}' —Å–æ—Ö—Ä–∞–Ω—ë–Ω/–æ–±–Ω–æ–≤–ª—ë–Ω!")

def main():
    print("üéôÔ∏è –ú–ê–°–¢–ï–† –°–û–ó–î–ê–ù–ò–Ø –≠–¢–ê–õ–û–ù–û–í –ì–û–õ–û–°–ê")
    print(f"üìÇ –§–∞–π–ª —ç—Ç–∞–ª–æ–Ω–æ–≤: {VOICEPRINTS_FILE}")
    
    inference = get_embedding_model()
    
    while True:
        print("\n" + "="*40)
        manager_name = input("–í–≤–µ–¥–∏—Ç–µ –∏–º—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ (–∏–ª–∏ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞): ").strip()
        if manager_name.lower() == 'exit': break
        if not manager_name: continue
        
        audio_path = input("–í–≤–µ–¥–∏—Ç–µ –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ mp3 —Ñ–∞–π–ª—É: ").strip()
        # –£–¥–∞–ª—è–µ–º –∫–∞–≤—ã—á–∫–∏, –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–∫–æ–ø–∏—Ä–æ–≤–∞–ª –ø—É—Ç—å –∫–∞–∫ "path"
        audio_path = audio_path.strip('"').strip("'")
        
        if not os.path.exists(audio_path):
            print("‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω!")
            continue
            
        embeddings, segments = extract_embeddings(audio_path, inference)
        
        if embeddings is None: continue

        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        print("   –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (K-Means)...")
        kmeans = KMeans(n_clusters=2, random_state=42).fit(embeddings)
        labels = kmeans.labels_
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã
        print("\nüîç –ö–¢–û –ï–°–¢–¨ –ö–¢–û?")
        
        print("\nüî¥ SPEAKER_0:")
        count = 0
        for i, seg in enumerate(segments):
            if labels[i] == 0:
                print(f"  [{seg.start:.1f}s]: {seg.text.strip()}")
                count += 1
                if count >= 3: break
                
        print("\nüîµ SPEAKER_1:")
        count = 0
        for i, seg in enumerate(segments):
            if labels[i] == 1:
                print(f"  [{seg.start:.1f}s]: {seg.text.strip()}")
                count += 1
                if count >= 3: break
        
        choice = input(f"\n–ö—Ç–æ –∏–∑ –Ω–∏—Ö {manager_name}? (0/1/skip): ").strip()
        
        if choice in ['0', '1']:
            target_label = int(choice)
            target_embs = embeddings[labels == target_label]
            voiceprint = np.mean(target_embs, axis=0)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            norm = np.linalg.norm(voiceprint)
            voiceprint = voiceprint / norm
            
            save_voiceprint(manager_name, voiceprint)
        else:
            print("‚è© –ü—Ä–æ–ø—É—â–µ–Ω–æ.")

if __name__ == "__main__":
    main()
