# -*- coding: utf-8 -*-
import os
import glob
import json
import time
import torch
import torchaudio
import numpy as np
from faster_whisper import WhisperModel
from pyannote.audio import Model, Inference
from sklearn.cluster import KMeans
from dotenv import load_dotenv
from scipy.spatial.distance import cosine

load_dotenv()

BASE_DIR = "/root/sales-ai-agent/data/archive"
STATUS_FILE = "/root/sales-ai-agent/data/system_status.json"
VOICEPRINTS_FILE = "/root/sales-ai-agent/data/voiceprints.json"
HF_TOKEN = os.getenv("HF_TOKEN")
EMBEDDING_MODEL_ID = "pyannote/wespeaker-voxceleb-resnet34-LM"

def update_status(progress_msg):
    try:
        if os.path.exists(STATUS_FILE):
            with open(STATUS_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
        else:
            data = {}
        data["process_progress"] = progress_msg
        with open(STATUS_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False)
    except Exception as e:
        print(f"Status error: {e}")

def load_voiceprints():
    if not os.path.exists(VOICEPRINTS_FILE):
        print("‚ö†Ô∏è –§–∞–π–ª voiceprints.json –Ω–µ –Ω–∞–π–¥–µ–Ω. –ë—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –º–µ—Ç–∫–∏ SPEAKER_XX.")
        return {}
    try:
        with open(VOICEPRINTS_FILE, 'r') as f:
            vps = json.load(f)
            print(f"üß† –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(vps)} —ç—Ç–∞–ª–æ–Ω–æ–≤ –≥–æ–ª–æ—Å–∞: {list(vps.keys())}")
            return vps
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è voiceprints.json: {e}")
        return {}

def identify_speakers(embeddings, labels, manager_name, voiceprints):
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∫—Ç–æ –∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (0 –∏–ª–∏ 1) —è–≤–ª—è–µ—Ç—Å—è –ú–µ–Ω–µ–¥–∂–µ—Ä–æ–º, —Å—Ä–∞–≤–Ω–∏–≤–∞—è —Å —ç—Ç–∞–ª–æ–Ω–æ–º.
    """
    # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —ç—Ç–∞–ª–æ–Ω –¥–ª—è —ç—Ç–æ–≥–æ –º–µ–Ω–µ–¥–∂–µ—Ä–∞
    if manager_name not in voiceprints:
        return {0: "SPEAKER_00", 1: "SPEAKER_01"}

    # 2. –ü–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä–∞
    target_vector = np.array(voiceprints[manager_name])
    
    cluster_0 = embeddings[labels == 0]
    cluster_1 = embeddings[labels == 1]
    
    # –ï—Å–ª–∏ –∫–∞–∫–æ–π-—Ç–æ –∫–ª–∞—Å—Ç–µ—Ä –ø—É—Å—Ç (–±—ã–≤–∞–µ—Ç –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–ø–∏—Å—è—Ö)
    if len(cluster_0) == 0: return {0: "UNKNOWN", 1: "–ú–µ–Ω–µ–¥–∂–µ—Ä"} # –°–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –≥–æ–≤–æ—Ä–∏–ª —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω
    if len(cluster_1) == 0: return {0: "–ú–µ–Ω–µ–¥–∂–µ—Ä", 1: "UNKNOWN"}

    center_0 = np.mean(cluster_0, axis=0)
    center_1 = np.mean(cluster_1, axis=0)
    
    # 3. –°—á–∏—Ç–∞–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (–º–µ–Ω—å—à–µ = –ª—É—á—à–µ)
    dist_0 = cosine(target_vector, center_0)
    dist_1 = cosine(target_vector, center_1)
    
    print(f"   üîç –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è {manager_name}: Dist(0)={dist_0:.3f}, Dist(1)={dist_1:.3f}")
    
    # 4. –ü—Ä–∏–Ω–∏–º–∞–µ–º —Ä–µ—à–µ–Ω–∏–µ
    if dist_0 < dist_1:
        # –ö–ª–∞—Å—Ç–µ—Ä 0 –±–ª–∏–∂–µ –∫ —ç—Ç–∞–ª–æ–Ω—É
        return {0: "–ú–µ–Ω–µ–¥–∂–µ—Ä", 1: "–ö–ª–∏–µ–Ω—Ç"}
    else:
        # –ö–ª–∞—Å—Ç–µ—Ä 1 –±–ª–∏–∂–µ –∫ —ç—Ç–∞–ª–æ–Ω—É
        return {1: "–ú–µ–Ω–µ–¥–∂–µ—Ä", 0: "–ö–ª–∏–µ–Ω—Ç"}

def diarize_audio(audio_path, segments, manager_name, voiceprints):
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–∫–µ—à–∏—Ä—É–µ—Ç—Å—è)
        model = Model.from_pretrained(EMBEDDING_MODEL_ID, use_auth_token=HF_TOKEN)
        inference = Inference(model, window="whole")
        
        # –†—É—á–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∞—É–¥–∏–æ —á–µ—Ä–µ–∑ torchaudio (–æ–±—Ö–æ–¥ –±–∞–≥–æ–≤ Pyannote)
        waveform, sample_rate = torchaudio.load(audio_path)
        
        # –†–µ—Å–µ–º–ø–ª–∏–Ω–≥ –≤ 16–∫–ì—Ü
        if sample_rate != 16000:
            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)
            waveform = resampler(waveform)
            sample_rate = 16000
            
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –º–æ–Ω–æ
        if waveform.shape[0] > 1:
            waveform = waveform.mean(dim=0, keepdim=True)

        embeddings = []
        valid_segments = []
        
        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º Whisper –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º –≤–µ–∫—Ç–æ—Ä–∞
        for seg in segments:
            if seg.end - seg.start < 0.2: continue # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            try:
                start = int(seg.start * sample_rate)
                end = int(seg.end * sample_rate)
                if end > waveform.shape[1]: end = waveform.shape[1]
                
                chunk = waveform[:, start:end]
                # –ü–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä
                emb = inference({"waveform": chunk, "sample_rate": sample_rate})
                if len(emb.shape) == 2: emb = emb[0]
                
                embeddings.append(emb)
                valid_segments.append(seg)
            except: pass

        if len(embeddings) < 2:
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –≤—ã–¥–µ–ª–∏—Ç—å 2 —Å–ø–∏–∫–µ—Ä–∞ -> —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ —ç—Ç–æ –æ–¥–∏–Ω —á–µ–ª–æ–≤–µ–∫ (–∏–ª–∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ)
            return [(s.start, s.end, "UNKNOWN", s.text) for s in segments]

        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (K-Means) –Ω–∞ 2 —Å–ø–∏–∫–µ—Ä–∞
        X = np.stack(embeddings)
        kmeans = KMeans(n_clusters=2, random_state=42).fit(X)
        labels = kmeans.labels_
        
        # –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è: –ö—Ç–æ –ú–µ–Ω–µ–¥–∂–µ—Ä?
        speaker_map = identify_speakers(X, labels, manager_name, voiceprints)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = []
        for i, seg in enumerate(valid_segments):
            role = speaker_map[labels[i]]
            result.append((seg.start, seg.end, role, seg.text))
            
        return result

    except Exception as e:
        print(f"‚ö†Ô∏è Diarization error: {e}")
        # Fallback: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç –±–µ–∑ —Ä–æ–ª–µ–π
        return [(s.start, s.end, "UNKNOWN", s.text) for s in segments]

def transcribe_all():
    print("üöÄ Starting Smart Transcription (Whisper + Voiceprint)...")
    update_status("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —ç—Ç–∞–ª–æ–Ω—ã
    voiceprints = load_voiceprints()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º Whisper
    model = WhisperModel("medium", device="cpu", compute_type="int8")
    
    all_files = []
    # –ü–æ–∏—Å–∫ –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ (–Ω–µ—Ç .txt)
    for week in sorted(os.listdir(BASE_DIR), reverse=True):
        week_path = os.path.join(BASE_DIR, week)
        if not os.path.isdir(week_path): continue
        for company in os.listdir(week_path):
            comp_path = os.path.join(week_path, company)
            if not os.path.isdir(comp_path): continue
            for manager in os.listdir(comp_path):
                mgr_path = os.path.join(comp_path, manager)
                audio_dir = os.path.join(mgr_path, "audio")
                trans_dir = os.path.join(mgr_path, "transcripts")
                
                if not os.path.exists(audio_dir): continue
                os.makedirs(trans_dir, exist_ok=True)
                
                mp3s = glob.glob(os.path.join(audio_dir, "*.mp3"))
                for mp3 in mp3s:
                    fname = os.path.basename(mp3)
                    txt_name = fname.replace(".mp3", ".txt")
                    txt_path = os.path.join(trans_dir, txt_name)
                    
                    if not os.path.exists(txt_path):
                        all_files.append({
                            "audio": mp3,
                            "txt": txt_path,
                            "manager": manager,
                            "file": fname
                        })

    total = len(all_files)
    if total == 0:
        print("‚úÖ –ù–µ—Ç –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏.")
        return

    print(f"üìÇ –ù–∞–π–¥–µ–Ω–æ {total} –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤.")
    
    for i, item in enumerate(all_files, 1):
        msg = f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {i}/{total}: {item['file']} ({item['manager']})..."
        print(msg)
        update_status(msg)
        
        try:
            # 1. Whisper (–¢–µ–∫—Å—Ç + –¢–∞–π–º–∫–æ–¥—ã)
            segments, _ = model.transcribe(item["audio"], vad_filter=True, beam_size=5, language="ru")
            seg_list = list(segments)
            
            # 2. –î–∏–∞—Ä–∏–∑–∞—Ü–∏—è + –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è (–∫—Ç–æ –µ—Å—Ç—å –∫—Ç–æ)
            diarized_segments = diarize_audio(item["audio"], seg_list, item["manager"], voiceprints)
            
            # 3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            with open(item["txt"], "w", encoding="utf-8") as f:
                for start, end, speaker, text in diarized_segments:
                    start_str = time.strftime('%M:%S', time.gmtime(start))
                    end_str = time.strftime('%M:%S', time.gmtime(end))
                    line = f"[{start_str} -> {end_str}] [{speaker}]: {text.strip()}\n"
                    f.write(line)
            
            print(f"   ‚úÖ –ì–æ—Ç–æ–≤–æ: {item['txt']}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ {item['file']}: {e}")
            update_status(f"–û—à–∏–±–∫–∞: {e}")

    update_status("–ì–æ—Ç–æ–≤–æ")

if __name__ == "__main__":
    transcribe_all()
